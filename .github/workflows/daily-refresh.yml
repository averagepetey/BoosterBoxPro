# Daily TCGplayer refresh (Apify + Listings Scraper)
# Runs at 10pm EST (03:00 UTC next day) with 16GB RAM - no 512Mi limit like Render free cron.
# Schedule: GitHub runs this at 03:00 UTC daily; re-push this file if scheduled runs stop.
#
# Required secrets (Repo → Settings → Secrets and variables → Actions):
#   DATABASE_URL          - DB for all metrics (Apify + Scraper write here).
#   APIFY_API_TOKEN       - Phase 1: sales/volume (boxes_sold_per_day, unified_volume_usd, boxes_sold_30d_avg).
#   BACKEND_URL           - Public API URL (e.g. https://your-api.run.app) so refresh can call invalidate-cache.
#   INVALIDATE_CACHE_SECRET - Secret for POST /admin/invalidate-cache; required for leaderboard/box detail to show new data automatically.
#
# What updates what (both phases write to DB; app reads from DB):
# - Phase 1 (Apify): sales/volume — boxes_sold_per_day, unified_volume_usd, floor_price, boxes_sold_30d_avg.
# - Phase 1b (eBay): eBay sold listings via Apify ($50/mo unlimited - epicscrapers actor).
# - Phase 2 (Scraper): listings + price — floor_price_usd, active_listings_count, boxes_added_today (yesterday→today delta).
# - Phase 3 (Rolling Metrics): Compute derived metrics (liquidity, days_to_20pct, EMAs) and upsert to DB.
# After all phases, the script commits updated historical_entries.json back to repo and calls cache invalidation.

name: Daily Refresh (Apify + Scraper)

on:
  schedule:
    # 10pm EST = 03:00 UTC next day (adjust if DST: 02:00 or 03:00)
    - cron: '0 3 * * *'
  workflow_dispatch: # allow manual run

jobs:
  daily-refresh:
    runs-on: ubuntu-latest
    timeout-minutes: 75

    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'

      - name: Cache pip
        uses: actions/cache@v4
        with:
          path: ~/.cache/pip
          key: ${{ runner.os }}-pip-${{ hashFiles('requirements.txt') }}

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt

      - name: Install Playwright Chromium
        run: playwright install chromium

      - name: Run daily refresh
        env:
          DATABASE_URL: ${{ secrets.DATABASE_URL }}
          APIFY_API_TOKEN: ${{ secrets.APIFY_API_TOKEN }}
          BACKEND_URL: ${{ secrets.BACKEND_URL }}
          INVALIDATE_CACHE_SECRET: ${{ secrets.INVALIDATE_CACHE_SECRET }}
        run: |
          if [ "${{ github.event_name }}" = "workflow_dispatch" ]; then
            python scripts/daily_refresh.py --no-delay
          else
            python scripts/daily_refresh.py
          fi

      - name: Commit updated historical data
        run: |
          git config --local user.email "github-actions[bot]@users.noreply.github.com"
          git config --local user.name "github-actions[bot]"
          git add data/historical_entries.json
          git diff --staged --quiet || git commit -m "Daily refresh: update historical data $(date -u +%Y-%m-%d)"
          git push
